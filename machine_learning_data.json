{
    "text": "Now that we had the ability to generate clean and reliable data, we were finally ready to do machine learning! This put the focus of the community squarely on the frameworks and abstractions to build machine learning models.\n\nEveryone was trying to do ML but what they needed were the right abstractions\nTools for applying ML techniques have existed for decades with many projects in R, MATLAB, and even Java. However, in most cases, early machine learning software and frameworks were focused on specific techniques \u2014 solving a new problem meant adopting a new tool and API. However, this all changed with the introduction of scikit-learn.\n\nWhile scikit-learn was introduced by INRIA in 2007 it wasn\u2019t until about 2012\u20132013 that it began to see widespread interest and adoption. This is precisely when data tools started to mature.\n\n\nThis Google search popularity trends chart shows that while scikit-learn was introduced in 2007, its popularity didn\u2019t take off until around 2012. Weirdly, around 2020 something else became more popular. Deep learning frameworks?\nWhat scikit-learn got right was a simple API for a wide range of models with sensible defaults and the ability to configure algorithm parameters. Furthermore, scikit-learn models could be composed to build pipelines within the same framework. Basically, scikit-learn worked out of the box, and it enabled you to improve the accuracy of your models as your skills grew. Other successful machine learning projects (e.g., XGBoost, Keras) followed this pattern to significant success.\n\nWhile the majority of real-world applications of ML continue to rely on scikit-learn and XGBoost, the focus of the community and the hype in machine learning has changed. If you go to any ML/AI research conference (turned trade-show) like NeurIPS, all you\u2019ll hear about is deep learning models built in TensorFlow, PyTorch, and (hopefully) Jax. The development and adoption of these frameworks paralleled the growth in interest in their simpler cousins \u2014 the combination of access to data & compute with simple abstractions was the key.\n\nDeep Learning was enabled by Data + Compute + Abstractions\nIn case you have been living under a statistics textbook from the 1990s, deep learning (the latest brand of neural networks) has achieved the state-of-the-art on a wide range of standard ML benchmarks. Deep learning is widely used for tasks like speech recognition, text-to-speech, computer vision, and machine translation.\n\nThe explosion of interest in deep learning was of course driven by the availability of high-quality data and access to affordable compute accelerators on public cloud hardware. If compute and data are the fuel of the deep learning revolution, it was the abstractions introduced by simple-yet-powerful software frameworks that ultimately catalyzed the explosive progress in deep learning.\n\nToday, if you can define a clear prediction task, then in a few lines of Python you can automatically apply a wide range of stochastic optimization algorithms on a number of hardware accelerators \u2014 all without having to derive a single equation or write a line of parallel code. Systems like Theano, Caffe, and later TensorFlow and PyTorch, helped accelerate innovation in model design by removing the need to address the mathematics or engineering that dominated early work in deep learning and machine learning systems. This has enabled everyone to join in the development of new deep learning techniques and has triggered a tsunami of deep learning research.\n\nIn short, the widespread access to data cleaning, distributed systems, and powerful abstractions has made machine learning into the revolutionary technology that it is today.\n\nWhat\u2019s next for Machine Learning?\nIn the last 15 years, we\u2019ve simultaneously enabled large-scale data processing, access to compute accelerators at cheap prices, and incredibly powerful software abstractions for ML. So\u2026 what\u2019s next?\n\nWhen I was preparing to go on the academic job market in 2015, it was clear that the massive investment in building models meant that we were going to have widespread of adoption machine learning models in the near future. I started asking myself what the next challenge was.\n\n\nThe concluding slide from my 2015 job talk depicts the cyclic interaction between data models and the actions we take based on those models. You know the slide is from 2015 because it is square. My argument then (and now) is that we as a community fixate on how data begets models and often forget the more important question of how models translate to actions and the consequences of those actions on future data.\nIf building models was about to be solved, it was clear that the next challenge would be how we used models \u2014 in other words, inference. But inference is easy\u2026 right? After all, who would build a machine learning model without knowing how they were going to use it?\n\nIn academia, we build models so that we can write papers. For us, inference is all about \u201ctesting\u201d our model or our idea. This is why we have training and testing as the two dominant stages of machine learning. Back in the real world, people use models to solve problems.\n\n\nThe broader data science lifecycle actually has many cycles in it but three distinct stages: model development, training, and deployment. The majority of the machine learning world is focused on learning which combines development and training. However, the opportunity for impact actually happens after learning when models are deployed to render predictions that drive actions and solve problems.\nReliably deploying and managing these trained models to render predictions in real-world settings would present entirely new systems challenges. Models deployed to improve business practices and augment user interactions would collect new data that could be used for automatic model selection, concept drift detection, and online learning and personalization \u2014 effectively opening up new avenues of machine learning research.\n\nUnlocking these next steps in machine learning innovation first requires the ability to consistently deliver high-quality predictions. Many of the biggest challenges in using machine learning today stem not from how we train models but instead from how we deliver these predictions to humans and applications and how we respond to feedback. This thesis has driven much of the research my group has done at Berkeley over the last 6 years and the company I launched with colleagues from Berkeley.\n\nIn the next post, I\u2019ll turn to some of the work (both from my group and from other awesome folks in ML systems) that has been done to tackle these problems. It\u2019s safe to say that, despite our best efforts, there\u2019s a lot of work left to be done.",
    "terms": [
        "machine learning (ml)", "frameworks", "abstractions", "models", "ml techniques", "data tools", "scikit-learn", "inria", "xgboost", 
    "keras", "deep learning", "neural networks", "tensorflow", "pytorch", "jax", "data processing", "compute accelerators", 
    "stochastic optimization algorithms", "parallel code", "theano", "caffe", "model design", "distributed systems", "high-quality data", 
    "cloud hardware", "model development", "training", "deployment", "inference", "data models", "predictions", "data science lifecycle", 
    "concept drift detection", "automatic model selection", "business practices", "user interactions", 
    "feedback", "real-world applications", "high-quality predictions", "model selection", "ml systems", "research", "data cleaning", 
    "benchmark", "speech recognition", "text-to-speech", "computer vision", "machine translation", "public cloud", "affordable compute", 
    "software abstractions", "model accuracy", "composable models", "simple api", "sensible defaults", "algorithm parameters", "data pipelines", 
    "data maturity", "trade show", "academic research", "standard ml benchmarks", "prediction task", 
    "mathematical derivation", "innovation", "model training", "data collection", "model deployment", "data feedback", "automatic learning", 
    "data cycles", "data begets models", "model actions", "data consequences", "model inference", "model testing", "model evaluation", 
    "training stage", "testing stage", "problem solving", "data augmentation",  
    "deployment challenges", "model management",  "data insights", "data-driven decisions", 
    "ml research", "academic job market", "future data", "model improvement", "ml lifecycle", "model personalization", 
     "data-driven applications"]
}